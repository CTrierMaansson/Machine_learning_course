---
title: "Demonstration of Clustering Methods"
author: "JAS"
date: 
output:
  html_document: default
  word_document: default
editor_options:
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


# Demonstration of Clustering Methods

We will be using the following dataset to demonstrate application and interpretation of clustering methods.

* Simulated data that we will use to represent clinical phenotypic data on COPD extracted from an EHR system. 
    + Data Citation: Ultsch, A.: Clustering with SOM: U*C, In Proc. Workshop on Self-Organizing Maps, Paris, France, (2005) , pp. 75-82

***

### Load Packages 
Ensure that all packages are installed before attempting to load. 

```{r load_packages}
library(dplyr)
library(stats)
library(factoextra)
library(cluster)

```

***
### Demonstration of Clustering Analysis
In this demonstration, we will attempt to uncover phenotypic subtypes within clinical data of Chronic Obstructive Pulmonary Disease (COPD). COPD is defined as airflow limitation that is not fully reversible. This is a very broad definition, and it suspected that there are a number of distinct phenotypes within the broader term of COPD. Identifying these subtypes can allow researchers to conduct more targeted investigations of COPD, uncovering mechanisms and risk factors for the different subtypes. This demonstration is loosely based on the work performed by Cho et al. Respiratory Research 2010; 11:30. The data are not the same. Please note that for practical reasons, we are using a small dataset with only 3 variables and 212 patient records. But, this same procedure could be repeated with a larger number of variables and/or records.

For this demonstration, the three variables in our dataset are:
1. post-bronchodilator FEV1 percent predicted
2. percent bronchodilator responsiveness
3. airway wall thickness

***

### Step 1: Load data and prepare for analysis
```{r dataprep}
setwd("C:/Users/chris/OneDrive/1PhD/Kurser-undervisning-supervision/Machine learning/R")
copd.data<-read.delim("./Hepta.lrn", header=FALSE)

#Strip off ID Variable
copd.data<-copd.data[,2:4]

#Assign Meaningful Variable Names
var.names<-c("pb_FEV1_pctpred", "pb_pct_br_resp", "awt")
colnames(copd.data)<-var.names

#Remove any observations with missing data
copd.data.nomiss<-na.omit(copd.data)

#Check means and SDs to determine if scaling is necessary
colMeans(copd.data.nomiss, na.rm=TRUE)
apply(copd.data.nomiss, 2, sd, na.rm=TRUE)

#Is scaling necessary?

```


### Step 2: Conduct a clustering analysis using k-means clustering
We can use the kmeans function in order to identify clusters within the data, based on the three variables.

We start by just randomly picking k=5 as the number of clusters. 
Then, we perform a more formal Gap Statistic Analysis to determine the optimal number of clusters

```{r kmeans}

set.seed(123)
clusters<-kmeans(copd.data.nomiss, 5, nstart=25)
str(clusters)
fviz_cluster(clusters, data=copd.data.nomiss)

#Show the mean value of features within each cluster
clusters$centers

#Conduct a gap_statistic analysis to determine optimal number of clusters
set.seed(123)
gap_stat<-clusGap(copd.data.nomiss, FUN=kmeans, nstart=25, K.max=9, B=10) #Det er 10 her, men burde være 100 eller 500. Det gøres for at have null reference population som altså er tilfældig cluster der holdes op mod de clusters man laver. Man vil have forskellen er så stor som muligt.
print(gap_stat, method="firstmax")
plot(gap_stat)

#Gap-statistic identifies 7 as the optimal number of clusters

clusters.7<-kmeans(copd.data.nomiss, 7, nstart=25)
str(clusters.7)
fviz_cluster(clusters.7, data=copd.data.nomiss)

#Interpret values within clusters
clusters.7$centers #Mean values of the features within the clusters

```

### Step 3: Conduct a hierarchical clustering analysis
Note there are different methods you can use to create your dissimilarity matrix. We are using complete linkage in this demonstration, which tends to produce more compact clusters. 
```{r hclustering}
set.seed(123)
# Create Dissimilarity matrix
diss.matrix <- dist(copd.data.nomiss, method = "euclidean")

# Hierarchical clustering using Complete Linkage
clusters.h<- hclust(diss.matrix, method = "complete" )

# Plot the obtained dendrogram
plot(clusters.h, cex = 0.6, hang = -1)

#create function to use within clusGap
hclusCut <- function(x, k) list(cluster = cutree(hclust(dist(x, method="euclidian"), method="complete"), k=k))

gap_stat <- clusGap(copd.data.nomiss, FUN = hclusCut, K.max = 10, B = 50)
fviz_gap_stat(gap_stat)


#Use number of clusters from gap statistic to obtain cluster assignment for each observation
clusters.h.7<-cutree(clusters.h, k=7)
table(clusters.h.7)

#Alternatives for hierarchical clustering=hcut function
set.seed(123)
clusters.hcut<-hcut(copd.data.nomiss, k=7, hc_func="hclust", hc_method="complete", hc_metric="euclidian")

clusters.hcut$size
fviz_dend(clusters.hcut, rect=TRUE)
fviz_cluster(clusters.hcut)

gap_stat <- clusGap(copd.data.nomiss, FUN = hcut, hc_method="complete", K.max = 10, B = 5)
fviz_gap_stat(gap_stat)

input.feature.vals<-cbind(copd.data.nomiss,cluster=clusters.hcut$cluster)

input.feature.vals %>%
  group_by(cluster) %>%
  summarise_all(mean)

#GENERAL SYNTAX
#input.feature.vals<-cbind(orig.data,cluster=cluster.object$cluster)

#input.feature.vals %>%
 # group_by(`cluster.object$cluster`) %>%
  #summarise_all(mean)

```


